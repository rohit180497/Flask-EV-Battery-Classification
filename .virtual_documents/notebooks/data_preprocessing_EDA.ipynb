# !pip3 install setuptools


# from pydantic_settings import BaseSettings



import pandas as pd
from datetime import datetime, timedelta
import datetime as dt
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
# import pandas_profiling as pp
import random 
import math

import scipy.stats as stats
import statsmodels.api as sm
from statsmodels.formula.api import ols
from scipy.stats import chi2_contingency
from scipy.stats import f_oneway
from scipy.stats import mannwhitneyu
from scipy.stats import pearsonr 

import warnings
warnings.filterwarnings('ignore')
# from sklearn.experimental import enable_iterative_imputer
# from sklearn.impute import IterativeImputer
from IPython.display import display_html


from sklearn.preprocessing import LabelEncoder,OneHotEncoder,StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.feature_selection import RFE





train_df = pd.read_csv('../data/raw_data.csv')


train_df.shape





from IPython.display import display, HTML

CSS = """
.output {
    flex-direction: row;
}
"""

HTML('<style>{}</style>'.format(CSS))


train_df.select_dtypes(include=['object']).shape


train_df.select_dtypes(exclude=['object']).shape


train_df.describe()


train_df.info()





train_df.isna().sum().head(20)


train_df.drop('Timestamp',1,inplace=True)


total = train_df.isnull().sum().sort_values(ascending=False)
percent = (total / len(train_df)) * 100
missing_df = pd.concat([total, percent], axis=1, keys=['Total', 'Percentage']).reset_index()


missing_df.drop('Total',1,inplace=True)


# df2_styler = missing_df.tail(20).style.set_table_attributes("style='display:inline'").set_caption('Caption table 2')
df1_styler = missing_df.head(15).style.set_table_attributes("style='display:inline'")
df2_styler = missing_df.iloc[15:30,:].style.set_table_attributes("style='display:inline'")
df3_styler = missing_df.iloc[30:45,:].style.set_table_attributes("style='display:inline'")
df4_styler = missing_df.tail(10).style.set_table_attributes("style='display:inline'")

display_html(df1_styler._repr_html_()+df2_styler._repr_html_()+df3_styler._repr_html_()+df4_styler._repr_html_(), raw=True)


cat_columns = train_df.select_dtypes(['object','category']).columns.to_list()
num_columns = train_df.select_dtypes(['int','float']).columns.to_list()


num_columns.remove('Battery_Health')
num_columns.remove('Battery_Class')


len(num_columns) + len(cat_columns) 


num_columns





from sklearn.impute import KNNImputer
# create an object for KNNImputer
imputer = KNNImputer(n_neighbors=5)
imputer.fit(train_df[num_columns])
train_df[num_columns] = imputer.transform(train_df[num_columns])


for col in train_df[cat_columns].columns:
    missing_len = train_df[col].isna().sum()
    li = train_df[~(train_df[col].isna())][col].to_list()
    train_df.loc[train_df[col].isna(),col] = [random.choice(li) for x in range(0, missing_len)]


# df2_styler = missing_df.tail(20).style.set_table_attributes("style='display:inline'").set_caption('Caption table 2')
total = train_df.isnull().sum().sort_values(ascending=False)
percent = (total / len(train_df)) * 100
missing_df = pd.concat([total, percent], axis=1, keys=['Total', 'Percentage']).reset_index()
missing_df.drop('Total',1, inplace=True)

df1_styler = missing_df.head(15).style.set_table_attributes("style='display:inline'")
df2_styler = missing_df.iloc[15:30,:].style.set_table_attributes("style='display:inline'")
df3_styler = missing_df.iloc[30:44,:].style.set_table_attributes("style='display:inline'")
df4_styler = missing_df.tail(10).style.set_table_attributes("style='display:inline'")

display_html(df1_styler._repr_html_()+df2_styler._repr_html_()+df3_styler._repr_html_()+df4_styler._repr_html_(), raw=True)


train_df['Battery_Class'].value_counts(normalize=True)


train_df[['Driving_Pattern','Weather_Conditions']].head(10)





# Loop through all columns
for col in train_df.columns:
    if train_df[col].dtype == 'object':
        train_df[col] = [str(x) for x in train_df[col]]
        # Check if each element in the column can be converted to a numeric value
        numeric_mask = pd.to_numeric(train_df[col], errors='coerce').notna()
        
        if numeric_mask.any():
            # Find the mode of the column excluding the numeric values
            mode_val = train_df.loc[~numeric_mask, col].mode()[0]
            
            # Replace numeric values with the mode
            train_df.loc[numeric_mask, col] = mode_val




train_df[['Driving_Pattern','Weather_Conditions']].head(10)





train_df = train_df.drop_duplicates(keep='last')





# Define a function to detect and treat outliers in each column
def treat_outliers(df, method='z-score', threshold=2):
    treated_df = df.copy()  # Create a copy of the original DataFrame
    outliers_info = {}  # Dictionary to store column names and outlier percentages
    
    for col in treated_df.columns:
        column_data = treated_df[col]
        if method == 'z-score':
            z_scores = np.abs((column_data - column_data.mean()) / column_data.std())
            outliers = z_scores > threshold
        elif method == 'IQR':
            Q1 = column_data.quantile(0.25)
            Q3 = column_data.quantile(0.75)
            IQR = Q3 - Q1
            lower_bound = Q1 - 1.5 * IQR
            upper_bound = Q3 + 1.5 * IQR
            outliers = (column_data < lower_bound) | (column_data > upper_bound)
        
        # Calculate the percentage of outliers in the column
        percent_outliers = (outliers.sum() / len(column_data)) * 100
        if percent_outliers > 0:
            outliers_info[col] = percent_outliers  # Store column name and outlier percentage
        
        # Replace outliers with the median of the column
        treated_df.loc[outliers, col] = column_data.median()
    
    return treated_df, outliers_info

# Apply the treat_outliers function to the entire DataFrame using z-score method
train_df[num_columns], outliers_info = treat_outliers(train_df[num_columns], method='z-score', threshold=2)




outliers_info


train_df['Battery_Class'].value_counts(normalize=True)


train_df.drop('Battery_Health',1,inplace=True)


train_df.info()









for col in train_df.columns:
    if train_df[col].dtype == 'float64':
        # For numeric columns, create histograms
        plt.figure(figsize=(8, 6))
        sns.histplot(train_df[col], bins=20, kde=True)
        plt.title(f'Distribution of {col}')
        plt.xlabel(col)
        plt.ylabel('Frequency')
        plt.show()
    elif train_df[col].dtype == 'object':
        # For categorical columns, create bar plots
        plt.figure(figsize=(8, 6))
        sns.countplot(data=train_df, x=col)
        plt.title(f'Distribution of {col}')
        plt.xlabel(col)
        plt.ylabel('Count')
        plt.xticks(rotation=45)
        plt.show()



#What is the distribution of the target variable 'Battery_Class'?

train_df['Battery_Class'].value_counts().plot(kind='bar')






#How balanced is the dataset in terms of the target variable?

train_df['Battery_Class'].value_counts(normalize=True)






#How do different 'Battery_Chemistry' types affect the distribution of 'Battery_Class'?


sns.countplot(data=train_df, x='Battery_Chemistry', hue='Battery_Class')


#Are there correlations between numeric features that might impact 'Battery_Class'?
train_df.corr()


def get_redundant_pairs(df):
    '''Get diagonal and lower triangular pairs of correlation matrix'''
    pairs_to_drop = set()
    cols = df.columns
    for i in range(0, df.shape[1]):
        for j in range(0, i+1):
            pairs_to_drop.add((cols[i], cols[j]))
    return pairs_to_drop

def get_top_abs_correlations(df, n=5):
    au_corr = df.corr().abs().unstack()
    labels_to_drop = get_redundant_pairs(df)
    au_corr = au_corr.drop(labels=labels_to_drop).sort_values(ascending=False)
    return au_corr[0:n]


get_top_abs_correlations(train_df.drop(cat_columns, axis=1),3)


#What is the relationship between 'State_of_Health' and 'Cycle_Life' for different 'Battery_Chemistry' types?


sns.lmplot(data=train_df, x='State_of_Health', y='Cycle_Life', hue='Battery_Chemistry')





#Are there any interactions between 'State_of_Charge' and 'Depth_of_Discharge' that affect 'Battery_Class'?

sns.scatterplot(data=train_df, x='State_of_Charge', y='Depth_of_Discharge', hue='Battery_Class')






#What is the distribution of 'State_of_Charge' for 'Battery_Class' 0 and 1?

sns.histplot(data=train_df[train_df['Battery_Class'] == 0], x='State_of_Charge', label='Battery_Class 0', kde=True)
sns.histplot(data=train_df[train_df['Battery_Class'] == 1], x='State_of_Charge', label='Battery_Class 1', kde=True)
plt.legend()



#Are there any outliers in 'Rate_of_Charge' for each 'Battery_Class' group?

sns.boxplot(data=train_df, x='Battery_Class', y='Rate_of_Charge')





#How do 'Cycling_Count' and 'Depth_of_Discharge' interact to influence 'Battery_Class'?

sns.scatterplot(data=train_df, x='Cycling_Count', y='Depth_of_Discharge', hue='Battery_Class')






#Is 'Charging_Frequency' related to 'Battery_Class'? Conduct a chi-square test.

from scipy.stats import chi2_contingency
contingency_table = pd.crosstab(train_df['Charging_Frequency'], train_df['Battery_Class'])
chi2, p, _, _ = chi2_contingency(contingency_table)


chi2_contingency(contingency_table)





#How does 'State_of_Health' vary with 'Battery_Chemistry'?

sns.violinplot(data=train_df, x='Battery_Chemistry', y='State_of_Health')






#What is the distribution of 'Cycle_Life' for different 'Battery_Chemistry' types?

sns.violinplot(data=train_df, x='Battery_Chemistry', y='Cycle_Life')



#How does 'Depth_of_Discharge' differ between 'Battery_Class' 0 and 1?

sns.violinplot(data=train_df[train_df['Battery_Class'].isin([0, 1])], x='Battery_Class', y='Depth_of_Discharge')






#Is there a significant difference in 'Rate_of_Charge' between different 'Battery_Chemistry' groups?

from scipy.stats import f_oneway
f_statistic, p_value = f_oneway(*[train_df['Rate_of_Charge'][train_df['Battery_Chemistry'] == chem] for chem in train_df['Battery_Chemistry'].unique()])
p_value





#Does 'State_of_Health' differ significantly between 'Battery_Class' 0 and 1?


import scipy.stats as stats

# Two-sample t-test for 'State_of_Health' by 'Battery_Class'
class_0 = train_df[train_df['Battery_Class'] == 0]['State_of_Health']
class_1 = train_df[train_df['Battery_Class'] == 1]['State_of_Health']
t_statistic, p_value = stats.ttest_ind(class_0, class_1)
print("P-value for 'State_of_Health' by 'Battery_Class':", p_value)
if p_value < 0.05:
    print("Inference: There is a significant difference in 'State_of_Health' between Battery_Class 0 and 1.")
else:
    print("Inference: There is no significant difference in 'State_of_Health' between Battery_Class 0 and 1.")





#Is there a significant difference in 'Cycle_Life' between different 'Battery_Class' values?

# One-way ANOVA for 'Cycle_Life' by 'Battery_Class'
f_statistic, p_value = f_oneway(train_df['Cycle_Life'][train_df['Battery_Class'] == 0], train_df['Cycle_Life'][train_df['Battery_Class'] == 1])
print("P-value for 'Cycle_Life' by 'Battery_Class':", p_value)
if p_value < 0.05:
    print("Inference: There is a significant difference in 'Cycle_Life' among Battery_Class groups.")
else:
    print("Inference: There is no significant difference in 'Cycle_Life' among Battery_Class groups.")





# Two-way ANOVA for 'Rate_of_Charge' by 'Battery_Class' and 'Battery_Chemistry'
model = ols('Rate_of_Charge ~ C(Battery_Class) * C(Battery_Chemistry)', data=train_df).fit()
anova_table = sm.stats.anova_lm(model, typ=2)
print("P-value for 'Rate_of_Charge' by 'Battery_Class' and 'Battery_Chemistry':", anova_table.loc['C(Battery_Class):C(Battery_Chemistry)', 'PR(>F)'])
if anova_table.loc['C(Battery_Class):C(Battery_Chemistry)', 'PR(>F)'] < 0.05:
    print("Inference: There is a significant interaction effect of 'Battery_Class' and 'Battery_Chemistry' on 'Rate_of_Charge'.")
else:
    print("Inference: There is no significant interaction effect of 'Battery_Class' and 'Battery_Chemistry' on 'Rate_of_Charge'.")






# Chi-square test for independence between 'Battery_Class' and 'State_of_Charge'
contingency_table = pd.crosstab(train_df['Battery_Class'], train_df['State_of_Charge'])
chi2, p_value, _, _ = chi2_contingency(contingency_table)
print("P-value for Chi-square test between 'Battery_Class' and 'State_of_Charge':", p_value)
if p_value < 0.05:
    print("Inference: 'Battery_Class' and 'State_of_Charge' are dependent variables.")
else:
    print("Inference: 'Battery_Class' and 'State_of_Charge' are independent variables.")



# Two-sample t-test for 'Cathode_Degradation' by 'Battery_Class' within each 'Battery_Chemistry' type
for chem in train_df['Battery_Chemistry'].unique():
    class_0 = train_df[(train_df['Battery_Class'] == 0) & (train_df['Battery_Chemistry'] == chem)]['Cathode_Degradation']
    class_1 = train_df[(train_df['Battery_Class'] == 1) & (train_df['Battery_Chemistry'] == chem)]['Cathode_Degradation']
    t_statistic, p_value = stats.ttest_ind(class_0, class_1)
    print(f"P-value for 'Cathode_Degradation' by 'Battery_Class' in 'Battery_Chemistry' {chem}:", p_value)
    if p_value < 0.05:
        print(f"Inference: There is a significant difference in 'Cathode_Degradation' between Battery_Class 0 and 1 for 'Battery_Chemistry' {chem}.")
    else:
        print(f"Inference: There is no significant difference in 'Cathode_Degradation' between Battery_Class 0 and 1 for 'Battery_Chemistry' {chem}.")












# Mann-Whitney U test for 'Depth_of_Discharge' by 'Battery_Class' within each 'Battery_Chemistry' type
for chem in train_df['Battery_Chemistry'].unique():
    class_0 = train_df[(train_df['Battery_Class'] == 0) & (train_df['Battery_Chemistry'] == chem)]['Depth_of_Discharge']
    class_1 = train_df[(train_df['Battery_Class'] == 1) & (train_df['Battery_Chemistry'] == chem)]['Depth_of_Discharge']
    U_statistic, p_value = mannwhitneyu(class_0, class_1, alternative='two-sided')
    print(f"P-value for 'Depth_of_Discharge' by 'Battery_Class' in 'Battery_Chemistry' {chem}:", p_value)
    if p_value < 0.05:
        print(f"Inference: There is a significant difference in 'Depth_of_Discharge' between Battery_Class 0 and 1 for 'Battery_Chemistry' {chem}.")
    else:
        print(f"Inference: There is no significant difference in 'Depth_of_Discharge' between Battery_Class 0 and 1 for 'Battery_Chemistry' {chem}.")






# Pearson correlation test for 'Rate_of_Charge' and 'State_of_Charge'
correlation_coefficient, p_value = pearsonr(train_df['Rate_of_Charge'], train_df['State_of_Charge'])
print("P-value for Pearson correlation test between 'Rate_of_Charge' and 'State_of_Charge':", p_value)
if p_value < 0.05:
    print("Inference: There is a significant positive correlation between 'Rate_of_Charge' and 'State_of_Charge'.")
else:
    print("Inference: There is no significant correlation between 'Rate_of_Charge' and 'State_of_Charge'.")








# Two-sample t-test for 'State_of_Charge' by 'Battery_Class' within each 'Charging_Infrastructure' group
for infrastructure in train_df['Charging_Infrastructure'].unique():
    class_0 = train_df[(train_df['Battery_Class'] == 0) & (train_df['Charging_Infrastructure'] == infrastructure)]['State_of_Charge']
    class_1 = train_df[(train_df['Battery_Class'] == 1) & (train_df['Charging_Infrastructure'] == infrastructure)]['State_of_Charge']
    t_statistic, p_value = stats.ttest_ind(class_0, class_1)
    print(f"P-value for 'State_of_Charge' by 'Battery_Class' in 'Charging_Infrastructure' {infrastructure}:", p_value)
    if p_value < 0.05:
        print(f"Inference: There is a significant difference in 'State_of_Charge' between Battery_Class 0 and 1 for 'Charging_Infrastructure' {infrastructure}.")
    else:
        print(f"Inference: There is no significant difference in 'State_of_Charge' between Battery_Class 0 and 1 for 'Charging_Infrastructure' {infrastructure}.")



data_backup = train_df.copy()


### Using Feature Selection Technique


#Weight of Evidence and Information Value and Multi-collinearity


from statsmodels.stats.outliers_influence import variance_inflation_factor


def calculate_vif(X):
    vif = pd.DataFrame()
    vif['variables'] = X.columns
    vif['VIF'] = [variance_inflation_factor (X.values, i) for i in range(X.shape[1])]
    return vif



calculate_vif(train_df[num_columns])





def cal_iv_num(df2, feature, target):
    df = df2.copy()
    print(feature)
    lst = []
    df[feature] = df[feature].replace([np.inf, -np.inf], np.nan)
    df[feature] = df[feature].fillna(0)
    df[feature] = pd.to_numeric(df[feature], errors='coerce')
    
    #Split continous variables into max 10 bins based on percentile
    df[feature+'_bin'] = pd.qcut(df[feature], q=100, duplicates='drop')
    for i in range(df[feature+'_bin'].nunique()):
        val = list(df[feature+'_bin'].unique())[i]
        lst.append([feature,
                    val,
                    df[df[feature+'_bin']==val].count()[feature],
                    df[(df[feature+'_bin']==val) & (df[target]==0)].count()[feature],
                    df[(df[feature+'_bin']==val) & (df[target]==1)].count()[feature] 
                    ])

    data = pd.DataFrame(lst, columns=['Variable', 'Value', 'All', 'Good', 'Bad'])
    data['Share'] = data['All']/data['All'].sum()
    data['Bad Rate'] = data['Bad'] / data['All']
    data['Distribution Good'] = (data['All'] - data['Bad'])/(data['All'].sum() - data['Bad'].sum())
    data['Distribution Bad'] = data['Bad']/data['Bad'].sum()
    data['WoE'] = np.log(data['Distribution Good']/data['Distribution Bad'])
    data = data.replace({'WoE':{np.inf:0, -np.inf:0}})
    data['IV_bin'] = data['WoE']*(data['Distribution Good'] - data['Distribution Bad'])
    data = data.sort_values(by=['Variable','Value'], ascending=[True, True])
    data.index = range(len(data.index))
    data['IV'] = data['IV_bin'].sum()

    return data


def cal_iv_cat(df2, feature, target):
    df = df2.copy()
    print(feature)
    lst = []
    #Ignore features having more than 100 unique values
    if df[feature].nunique()>100:
        return pd.DataFrame()

    df[feature] = df[feature].fillna(0)
    df[feature+'_bin'] = df[feature]

    for i in range(df[feature+'_bin'].nunique()):
        val = list(df[feature+'_bin'].unique())[i]

        lst.append([feature,
                    val,
                    df[df[feature+'_bin']==val].count()[feature],
                    df[(df[feature+'_bin']==val) & (df[target]==0)].count()[feature],
                    df[(df[feature+'_bin']==val) & (df[target]==1)].count()[feature] 
                    ])
    
    data = pd.DataFrame(lst, columns=['Variable', 'Value', 'All', 'Good', 'Bad'])
    data['Share'] = data['All']/data['All'].sum()
    data['Bad Rate'] = data['Bad'] / data['All']
    data['Distribution Good'] = (data['All'] - data['Bad'])/(data['All'].sum() - data['Bad'].sum())
    data['Distribution Bad'] = data['Bad']/data['Bad'].sum()
    data['WoE'] = np.log(data['Distribution Good']/data['Distribution Bad'])
    data = data.replace({'WoE':{np.inf:0, -np.inf:0}})
    data['IV_bin'] = data['WoE']*(data['Distribution Good'] - data['Distribution Bad'])
    data = data.sort_values(by=['Variable','Value'], ascending=[True, True])
    data.index = range(len(data.index))
    data['IV'] = data['IV_bin'].sum()

    return data


def iterate_vars_iv(df, Cat_vars, target):
    data = pd.DataFrame()

    varlist = list(df.columns)

    for i in pd.Series(varlist):
        if str(i) in Cat_vars:
            data = data.append(cal_iv_cat(df, str(i), target))
        else:
            data = data.append(cal_iv_num(df, str(i), target))

    return data


iv_df = iterate_vars_iv(train_df, cat_columns,'Battery_Class')


iv_df[['Variable','WoE','IV']].drop_duplicates(subset = 'Variable', keep='last').reset_index(drop=True)


le = LabelEncoder()

for col in train_df[cat_columns].columns:
    train_df[col] = le.fit_transform(train_df[col])
    train_df[col] = train_df[col].astype('category')


X_train = train_df.drop(['Battery_Class'], axis=1)


y_train = train_df['Battery_Class']


classifier= RandomForestClassifier(n_estimators=100, max_depth=10)


n_features_to_select = 15
rfe = RFE(classifier, n_features_to_select = n_features_to_select)
rfe.fit(X_train, y_train)


from operator import itemgetter
features = X_train.columns.to_list()
for x, y,z in (sorted(zip(rfe.support_,rfe.ranking_ , features), key=itemgetter(0))):
    print(x,'__Rank',y,  '__' ,z) 


Feature_importance_DF = pd.DataFrame(zip(rfe.support_,rfe.ranking_ ,features),columns=['Support','Rank','Columns'])


Important_Features = Feature_importance_DF[Feature_importance_DF['Rank']==1]['Columns'].to_list()


print(Important_Features)


from sklearn.tree import DecisionTreeClassifier

clf = DecisionTreeClassifier()
clf.fit(X_train, y_train)

pd.Series(clf.feature_importances_, index=X_train.columns[:]).plot.bar(color='steelblue', figsize=(12, 6))


from pycaret.classification import * 


exp_clf = setup(train_df, target='Battery_Class',imputation_type='iterative',fix_imbalance = True)


top3 = compare_models()





train_df.to_csv('../data/Processed_Data.csv', index=False)




































